{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Training data\n",
    "# Input, 0 = bumpy, 1 = smooth\n",
    "features = [[140, 1], [130, 1], [150, 0], [170, 0]]\n",
    "# Output, 0 = apple, 1 = orange\n",
    "labels = [0, 0, 1, 1]\n",
    "\n",
    "# Defines type of classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Defines our Learning Algorithm (fit is for \"Finding patterns in Data\")\n",
    "clf = clf.fit(features, labels)\n",
    "print(clf.predict([[140, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import data set\n",
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "'''\n",
    "# print info from set\n",
    "print(iris.feature_names) # metadata\n",
    "print(iris.target_names)\n",
    "print(iris.data[0]) # features\n",
    "print(iris.target[0]) # labels\n",
    "'''\n",
    "\n",
    "testIdx = [0, 50, 100]  # each index where a new set of flowers begin\n",
    "\n",
    "# training data\n",
    "trainTarget = np.delete(iris.target, testIdx)\n",
    "trainData = np.delete(iris.data, testIdx, axis=0)\n",
    "\n",
    "# testing data\n",
    "testTarget = iris.target[testIdx]\n",
    "testData = iris.data[testIdx]\n",
    "\n",
    "# classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(trainData, trainTarget)\n",
    "\n",
    "print(testTarget)  # what our label is expected to be\n",
    "print(clf.predict(testData))\n",
    "\n",
    "print(testData[0], testTarget[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "greyhounds = 500  # sample population\n",
    "labs = 500\n",
    "\n",
    "greyHeight = 28 + 4 * np.random.randn(\n",
    "    greyhounds)  # change to make it realistic for actual data, but random ints added to increase variation\n",
    "labHeight = 24 + 4 * np.random.randn(labs)  # random adds height to entire population\n",
    "\n",
    "plt.hist([greyHeight, labHeight], stacked=True, color=['r', 'b'])  # Histogram to represent data\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def euc(a, b):\n",
    "    return distance.euclidean(a, b)  # calculate point distance\n",
    "\n",
    "\n",
    "class ScrappyKNN():  # Our classifier (Scrappy = bare-bones)\n",
    "    def fit(self, x_train, y_train):  # takes the features and labels as input\n",
    "        self.x_train = x_train  # features\n",
    "        self.y_train = y_train  # labels\n",
    "\n",
    "    def predict(self, x_test):  # inputs the features from testing data and outputs predictions\n",
    "        predictions = []  # returning list of predictions\n",
    "        for row in x_test:\n",
    "            label = self.closest(row)  # finds closest training point from test point\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "\n",
    "    def closest(self, row):  # loops over all of training points and updates to the closest one so far\n",
    "        bestDist = euc(row, self.x_train[0])  # records best distance from testing point to training point\n",
    "        bestIndex = 0  # records the best index\n",
    "        for i in range(len(self.x_train)):\n",
    "            dist = euc(row, self.x_train[i])\n",
    "            if dist < bestDist:\n",
    "                bestDist = dist\n",
    "                bestIndex = i\n",
    "        return self.y_train[bestIndex]\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x = iris.data  # features\n",
    "y = iris.target  # labels\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# a built in function that allows for automated train/test splitting of the data.\n",
    "# features and label values are used, and 'test_size' defines how much of the data is split into train or test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.5)\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier # train a new type of classifier\n",
    "clf = ScrappyKNN()  # sets classifier var to class\n",
    "\n",
    "\n",
    "def results(prediction):  # converts results to readable strings\n",
    "    data = prediction\n",
    "    for i in range(len(data)):\n",
    "        if predict[i] == 0:\n",
    "            data[i] = \"setosa\"\n",
    "        elif predict[i] == 1:\n",
    "            data[i] = \"versicolor\"\n",
    "        elif predict[i] == 2:\n",
    "            data[i] = \"virginica\"\n",
    "    return data\n",
    "\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "predict = clf.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, predict))\n",
    "\n",
    "results = results(predict)\n",
    "print(results)  # this prints the type of iris the classifier predicts for each row of testing data\n",
    "\n",
    "'''\n",
    "customData = [5.1, 3.5, 1.4, 0.2]\n",
    "customPredict = clf.predict(customData)\n",
    "customResults = results(customPredict)\n",
    "print(customResults)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FORKED FROM @TENSORFLOW/TENSORFLOW. DO NOT DISTRIBUTE WITHOUT EDITING.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "learn = tf.contrib.learn\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Import the dataset\n",
    "mnist = learn.datasets.load_dataset('mnist')\n",
    "data = mnist.train.images\n",
    "labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "test_data = mnist.test.images\n",
    "test_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "max_examples = 10000\n",
    "data = data[:max_examples]\n",
    "labels = labels[:max_examples]\n",
    "\n",
    "\n",
    "# Display some digits\n",
    "def display(i):\n",
    "    img = test_data[i]\n",
    "    plt.title('Example %d. Label: %d' % (i, test_labels[i]))\n",
    "    plt.imshow(img.reshape((28, 28)), cmap=plt.cm.gray_r)\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    display(i)\n",
    "\n",
    "# lists features\n",
    "print(len(data[0]))\n",
    "\n",
    "# Fit a Linear Classifier\n",
    "feature_columns = learn.infer_real_valued_columns_from_input(data)\n",
    "classifier = learn.LinearClassifier(feature_columns=feature_columns, n_classes=10)\n",
    "classifier.fit(data, labels, batch_size=100, steps=1000)\n",
    "\n",
    "# Evaluate accuracy\n",
    "results = classifier.evaluate(test_data, test_labels)\n",
    "print(results, \"Accuracy\")\n",
    "\n",
    "predict = classifier.predict((test_data[0]), (test_labels[0]))\n",
    "\n",
    "# here's one it gets right\n",
    "print(\"Predicted %d, Label: %d\" % (predict))\n",
    "display(0)\n",
    "# one it gets wrong\n",
    "predict = classifier.predict((test_data[8]), (test_labels[8]))\n",
    "\n",
    "print(\"Predicted %d, Label: %d\" % (predict))\n",
    "display(8)\n",
    "\n",
    "# Let's see if we can reproduce the pictures of the weights in the TensorFlow Basic MNSIT\n",
    "weights = classifier.weights_\n",
    "f, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "axes = axes.reshape(-1)\n",
    "for i in range(len(axes)):\n",
    "    a = axes[i]\n",
    "    a.imshow(weights.T[i].reshape(28, 28), cmap=plt.cm.seismic)\n",
    "    a.set_title(i)\n",
    "    a.set_xticks(())  # ticks be gone\n",
    "    a.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Python 2 / 3 compatability\n",
    "from __future__ import print_function\n",
    "\n",
    "# Toy dataset.\n",
    "# Format: each row is an example.\n",
    "# The last column is the label.\n",
    "# The first two columns are features.\n",
    "training_data = [\n",
    "    [\"Green\", 3, \"Apple\"],\n",
    "    [\"Yellow\", 3, \"Apple\"],\n",
    "    [\"Red\", 1, \"Grape\"],\n",
    "    [\"Red\", 1, \"Grape\"],\n",
    "    [\"Yellow\", 3, \"Lemon\"],\n",
    "    [\"Red\", 3, \"Apple\"],\n",
    "    [\"Red\", 3, \"Apple\"],\n",
    "]\n",
    "\n",
    "# print the tree\n",
    "header = [\"color\", \"diameter\", \"label\"]\n",
    "\n",
    "\n",
    "# print(header)\n",
    "\n",
    "\n",
    "def unique_vals(rows, col):\n",
    "    # Find the unique values for a column in a dataset.\n",
    "    return set([row[col] for row in rows])\n",
    "\n",
    "\n",
    "# unique_vals(training_data, 0)\n",
    "# returns column 0 of training data\n",
    "\n",
    "def class_counts(rows):\n",
    "    # Counts the number of each type of example in a dataset.\n",
    "    counts = {}  # a dictionary of label -> count.\n",
    "    for row in rows:\n",
    "        # in our dataset format, the label is always the last column\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "# class_counts(training_data)\n",
    "# returns how many times a label is present\n",
    "\n",
    "def is_numeric(value):\n",
    "    \"\"\"Test if a value is numeric. (E.g Distinguish color names from diameter)\"\"\"\n",
    "    return isinstance(value, int) or isinstance(value, float)\n",
    "\n",
    "\n",
    "# Demo\n",
    "# is_numeric(7)\n",
    "\n",
    "# Represents how a question is determined\n",
    "class Question:\n",
    "    \"\"\"A Question is used to partition a dataset.\n",
    "\n",
    "    This class just records a 'column number' (e.g., 0 for Color) and a\n",
    "    'column value' (e.g., Green). The 'match' method is used to compare\n",
    "    the feature value in an example to the feature value stored in the\n",
    "    question. See the demo below.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column, value):\n",
    "        # stores for the threshold used to partition the data\n",
    "        self.column = column  # string of type of feature\n",
    "        self.value = value  # value of the given node in the table\n",
    "\n",
    "    def match(self, example):\n",
    "        # Compare the feature value in an example to the\n",
    "        # feature value in this question.\n",
    "        val = example[self.column]\n",
    "        # print(val) prints the feature at self.column and the given index of training_data\n",
    "        if is_numeric(val):  # returns boolean values\n",
    "            return val >= self.value\n",
    "        else:\n",
    "            return val == self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        # This is just a helper method to print\n",
    "        # the question in a readable format.\n",
    "        condition = \"==\"\n",
    "        if is_numeric(self.value):\n",
    "            condition = \">=\"\n",
    "        return \"Is %s %s %s?\" % (\n",
    "            header[self.column], condition, str(self.value))\n",
    "\n",
    "\n",
    "# Prints example questions\n",
    "q = Question(0, 'Green')  # Is color == Green?\n",
    "print(q)\n",
    "print(Question(1, 3))  # Is diameter >= 3?\n",
    "\n",
    "# An example from the training set to see if it matches the question\n",
    "example = training_data[0]\n",
    "print(q.match(example))  # this will be true, since the first example is Green.\n",
    "\n",
    "\n",
    "def partition(rows, question):\n",
    "    \"\"\"Partitions a dataset.\n",
    "\n",
    "    For each row in the dataset, check if it matches the question. If\n",
    "    so, add it to 'true rows', otherwise, add it to 'false rows'.\n",
    "    \"\"\"\n",
    "    true_rows, false_rows = [], []\n",
    "    for row in rows:  # If the question is true for the given data, place it in true, otherwise it's false\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "    return true_rows, false_rows\n",
    "\n",
    "\n",
    "# Partition the training data based on whether rows are Red.\n",
    "true_rows, false_rows = partition(training_data, Question(0, 'Red'))\n",
    "\n",
    "\n",
    "# print(true_rows) Contains rows with Red\n",
    "\n",
    "def gini(rows):\n",
    "    \"\"\"Calculate the Gini Impurity for a list of rows.\n",
    "\n",
    "    There are a few different ways to do this, this one was\n",
    "    the most concise. See:\n",
    "    https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n",
    "    \"\"\"\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl ** 2\n",
    "    return impurity\n",
    "\n",
    "\n",
    "# First, we'll look at a dataset with no mixing.\n",
    "no_mixing = [['Apple'],\n",
    "             ['Apple']]\n",
    "# this will return 0\n",
    "print(gini(no_mixing))\n",
    "\n",
    "some_mixing = [['Apple'],\n",
    "               ['Orange']]\n",
    "# this will return 0.5 - meaning, there's a 50% chance of misclassifying\n",
    "# a random example we draw from the dataset.\n",
    "print(gini(some_mixing))\n",
    "\n",
    "lots_of_mixing = [['Apple'],\n",
    "                  ['Orange'],\n",
    "                  ['Grape'],\n",
    "                  ['Grapefruit'],\n",
    "                  ['Blueberry']]\n",
    "# This will return 0.8\n",
    "print(gini(lots_of_mixing))\n",
    "\n",
    "\n",
    "def info_gain(left, right, current_uncertainty):\n",
    "    \"\"\"Information Gain.\n",
    "\n",
    "    The uncertainty of the starting node, minus the weighted impurity of\n",
    "    two child nodes.\n",
    "    \"\"\"\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_uncertainty - p * gini(left) - (1 - p) * gini(right)\n",
    "\n",
    "\n",
    "# Calculate the uncertainy of our training data.\n",
    "current_uncertainty = gini(training_data)\n",
    "print(current_uncertainty)\n",
    "\n",
    "\n",
    "def find_best_split(rows):\n",
    "    \"\"\"Find the best question to ask by iterating over every feature / value\n",
    "    and calculating the information gain.\"\"\"\n",
    "    best_gain = 0  # keep track of the best information gain\n",
    "    best_question = None  # keep train of the feature / value that produced it\n",
    "    current_uncertainty = gini(rows)\n",
    "    n_features = len(rows[0]) - 1  # number of columns\n",
    "\n",
    "    for col in range(n_features):  # for each feature\n",
    "\n",
    "        values = set([row[col] for row in rows])  # unique values in the column\n",
    "\n",
    "        for val in values:  # for each value\n",
    "\n",
    "            question = Question(col, val)\n",
    "\n",
    "            # try splitting the dataset\n",
    "            true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "            # Skip this split if it doesn't divide the\n",
    "            # dataset.\n",
    "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate the information gain from this split\n",
    "            gain = info_gain(true_rows, false_rows, current_uncertainty)\n",
    "\n",
    "            # You actually can use '>' instead of '>=' here\n",
    "            # but I wanted the tree to look a certain way for our\n",
    "            # toy dataset.\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_question = gain, question\n",
    "\n",
    "    return best_gain, best_question\n",
    "\n",
    "\n",
    "# Find the best question to ask first for our toy dataset.\n",
    "best_gain, best_question = find_best_split(training_data)\n",
    "print(best_question)\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    \"\"\"A Leaf node classifies data.\n",
    "\n",
    "    This holds a dictionary of class (e.g., \"Apple\") -> number of times\n",
    "    it appears in the rows from the training data that reach this leaf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = class_counts(rows)\n",
    "\n",
    "\n",
    "class Decision_Node:\n",
    "    \"\"\"A Decision Node asks a question.\n",
    "\n",
    "    This holds a reference to the question, and to the two child nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 true_branch,\n",
    "                 false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "\n",
    "\n",
    "def build_tree(rows):\n",
    "    \"\"\"Builds the tree.\n",
    "\n",
    "    Rules of recursion: 1) Believe that it works. 2) Start by checking\n",
    "    for the base case (no further information gain). 3) Prepare for\n",
    "    giant stack traces.\n",
    "    \"\"\"\n",
    "\n",
    "    # Try partitioing the dataset on each of the unique attribute,\n",
    "    # calculate the information gain,\n",
    "    # and return the question that produces the highest gain.\n",
    "    gain, question = find_best_split(rows)\n",
    "\n",
    "    # Base case: no further info gain\n",
    "    # Since we can ask no further questions,\n",
    "    # we'll return a leaf.\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "\n",
    "    # If we reach here, we have found a useful feature / value\n",
    "    # to partition on.\n",
    "    true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "    # Recursively build the true branch.\n",
    "    true_branch = build_tree(true_rows)\n",
    "\n",
    "    # Recursively build the false branch.\n",
    "    false_branch = build_tree(false_rows)\n",
    "\n",
    "    # Return a Question node.\n",
    "    # This records the best feature / value to ask at this point,\n",
    "    # as well as the branches to follow\n",
    "    # dependingo on the answer.\n",
    "    return Decision_Node(question, true_branch, false_branch)\n",
    "\n",
    "\n",
    "# Recieves the entire training set as input\n",
    "my_tree = build_tree(training_data)\n",
    "print(my_tree)\n",
    "\n",
    "\n",
    "def classify(row, node):\n",
    "    \"\"\"See the 'rules of recursion' above.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.predictions\n",
    "\n",
    "    # Decide whether to follow the true-branch or the false-branch.\n",
    "    # Compare the feature / value stored in the node,\n",
    "    # to the example we're considering.\n",
    "    if node.question.match(row):\n",
    "        return classify(row, node.true_branch)\n",
    "    else:\n",
    "        return classify(row, node.false_branch)\n",
    "\n",
    "\n",
    "# training data is an apple with confidence 1.\n",
    "print(classify(training_data[0], my_tree))\n",
    "\n",
    "\n",
    "def print_leaf(counts):\n",
    "    \"\"\"A nicer way to print the predictions at a leaf.\"\"\"\n",
    "    total = sum(counts.values()) * 1.0\n",
    "    probs = {}\n",
    "    for lbl in counts.keys():\n",
    "        probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
    "    return probs\n",
    "\n",
    "\n",
    "# Printing that a bit nicer\n",
    "print(print_leaf(classify(training_data[0], my_tree)))\n",
    "\n",
    "# Evaluate\n",
    "testing_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 4, 'Apple'],\n",
    "    ['Red', 2, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "]\n",
    "\n",
    "for row in testing_data:\n",
    "    print(\"Actual: %s. Predicted: %s\" %\n",
    "          (row[-1], print_leaf(classify(row, my_tree))))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
