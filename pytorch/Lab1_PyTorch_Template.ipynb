{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools Setup and Determine Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Classes GPU_Dataset.py for dataset inspection in GPU environment and TinyImageNetModel.py for training a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from tinyimagenet import TinyImageNet\n",
    "from TinyImageNetModel import TinyImageNetModel, init_weights\n",
    "from pathlib import Path\n",
    "from GPU_Dataset import GPUDS\n",
    "\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a training and validation set and view data about sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports our dataset.\n",
    "\n",
    "# Original Source: https://github.com/ksachdeva/tiny-imagenet-tfds\n",
    "# Setup our dataset\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Training dataset\n",
    "dataset_path=\"~/.torchvision/tinyimagenet/\"\n",
    "ds_train = TinyImageNet(Path(dataset_path),split=\"train\")\n",
    "n = len(ds_train)\n",
    "print(f\"TinyImageNet, split \\\"val\\\", has {n} samples.\")\n",
    "samples_to_print = 2\n",
    "print(f\"Showing info of {samples_to_print} samples...\")\n",
    "for i in range(0,n,n//samples_to_print):\n",
    "    image, img_class = ds_train[i]\n",
    "    print(f\"Sample of class {img_class:3d}\\n Shape {image.shape}\\n Image Data {image}\\n Labels {ds_train.idx_to_words[img_class]}\")\n",
    "    # by default, [C, H, W], transpose to [W, H, C]\n",
    "    image = np.transpose(image, (2, 1, 0))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Validation dataset\n",
    "ds_val = TinyImageNet(Path(dataset_path),split=\"val\")\n",
    "n = len(ds_val)\n",
    "print(f\"TinyImageNet, split \\\"val\\\", has {n} samples.\")\n",
    "samples_to_print = 2\n",
    "print(f\"Showing info of {samples_to_print} samples...\")\n",
    "for i in range(0,n,n//samples_to_print):\n",
    "    image, img_class = ds_val[i]\n",
    "    print(f\"Sample of class {img_class:3d}\\n Shape {image.shape}\\n Image Data {image}\\n Labels {ds_train.idx_to_words[img_class]}\")\n",
    "    # by default, [C, H, W], transpose to [W, H, C]\n",
    "    image = np.transpose(image, (2, 1, 0))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export three random images to use as input feature maps for C++ framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Export each of the three inputs to a binary file which will be used to load the images into C++ later\n",
    "# NOTE: First flatten the array (ex: 4D --> 1D). So 64*64*3 = 12288 element 1D array\n",
    "\n",
    "# TODO: Print and visualize three inputs from the validation set\n",
    "#     : Print the stroage data type\n",
    "#     : Print and note the dimensions of each image\n",
    "#     : Print the memory required to store each image\n",
    "\n",
    "# Make a directory for our image data\n",
    "img_dir = os.path.abspath('img_data')\n",
    "pathlib.Path(img_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# Dataloaders\n",
    "val_dataloader = DataLoader(ds_val, batch_size=1, shuffle=True)\n",
    "train_dataloader = DataLoader(ds_train, batch_size=1, shuffle=True)\n",
    "\n",
    "i = 0\n",
    "samples = []\n",
    "# [num images in batch, channels, width, height] by default\n",
    "for image, label in val_dataloader:\n",
    "    if i >= 3:\n",
    "        break\n",
    "    else:\n",
    "        samples.append((image, label))\n",
    "        # Export sample images\n",
    "        # TODO: Your Code Here\n",
    "        \n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the model\n",
    "# Now we will load the PT model! Please make sure the pt model file is present in the below directory.\n",
    "# You can download this from the Canvas Page and place it in the same directory as this notebook.\n",
    "\n",
    "# model_path = os.path.abspath(\"\"/home/<NETID>/path/to/your/lab1/tinyimagenet_model.pt)\" # Uncomment this to use a non-relative path\n",
    "model_path = os.path.abspath(\"./tinyimagenet_model.pt\")\n",
    "\n",
    "# TODO: Your Code Here\n",
    "#model =\n",
    "\n",
    "# TODO: Print a summary of the model structure\n",
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference on Selected 3 Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running infrence on our model\n",
    "# We can run an infrence of our model by doing the following (we are doing batches of 1 here)\n",
    "image = image.to(device)\n",
    "# print(image)\n",
    "pred = model(image)\n",
    "pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "top1_guess = torch.max(pred, dim=1)\n",
    "# torch.max() returns [value, label_index]\n",
    "top1_label = ds_val.idx_to_words[top1_guess[1].item()]\n",
    "actual_label = ds_val.idx_to_words[label.item()]\n",
    "print(f'Guess: {top1_label} Confidence %: {top1_guess[0].item() * 100} // Actual: {actual_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run infrence for our previous 3 sample images\n",
    "# NOTE: Turn off gradient computation for inference mode\n",
    "\n",
    "# TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Accuracy of Top-1, Top-5, and Top-10 classifications of Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the Top-1, Top-5, and Top-10 Accuracy of the validation dataset\n",
    "total = acc_top1 = acc_top5 = acc_top10 = 0\n",
    "\n",
    "# TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print All Dataset Classes and their Associated Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print all of the possible classes of the dataset\n",
    "\n",
    "# TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the model layers and filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the model in Netron (https://netron.app/) and include an image here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Layer Weigths\n",
    "# Get the first and last Conv2d layers\n",
    "#conv2d = model.tinyimgnet_model[0]\n",
    "\n",
    "# We can view the layer weights. Here we consider them as images of feature filters applied to intermediate feature map images.\n",
    "# TODO: Visualize the 2 convolutional layers filter sets (weights) (one at the beginning and one at the end)\n",
    "\n",
    "# TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view the layer outputs as well. Here we consider them as images of the spatial location of features.\n",
    "# TODO: Visualize the 2 convolutional layers outputs (intermediate feature maps) (one at the beginning and one at the end)\n",
    "\n",
    "# TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Export the filters/weights se we can use them later\n",
    "# Make a directory for our image data\n",
    "model_dir = os.path.abspath('model_data')\n",
    "pathlib.Path(model_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# Export each layer's weights and biases\n",
    "# These are the convolutional and linear (dense) layers\n",
    "for layer_num, layer in enumerate(model.tinyimgnet_model):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        layer_type = \"Linear\"\n",
    "    elif isinstance(layer, nn.Conv2d):\n",
    "        layer_type = \"Conv2d\"\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    # TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Intermediate Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Export the intermediate layer outputs for each of the input for all of the layers\n",
    "img_dir = os.path.abspath('img_data')\n",
    "pathlib.Path(img_dir).mkdir(exist_ok=True)\n",
    "\n",
    "for img_idx, img in enumerate(sample_imgs):\n",
    "    file_dir = os.path.join(img_dir, f'test_input_{img_idx}')\n",
    "    pathlib.Path(file_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Performance Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile One Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather grab a profiling trace on one of the sample images\n",
    "\n",
    "activities = [ProfilerActivity.CPU]\n",
    "with profile(activities=activities, record_shapes=True) as prof:\n",
    "    with record_function(\"end_to_end_inference\"):\n",
    "        model(samples[0][0].to(device)) \n",
    "\n",
    "# opening this with perfetto ui is actually pretty neat and way easier than tensorboard\n",
    "prof.export_chrome_trace(\"image_0_trace.json\")\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sample Profiling - Inference for a single image:\n",
    "\n",
    "# Perform the inference profiling:\n",
    "samples_to_run = [10, 100, 1000]\n",
    "\n",
    "val_dataloader = DataLoader(ds_val, batch_size=1, shuffle=True)\n",
    "\n",
    "for samples_num in samples_to_run:\n",
    "    # Engage profiling\n",
    "    with profile(activities=activities) as prof:\n",
    "        with record_function(f\"e2e_online_{samples_num}\"):\n",
    "            # Actual inference\n",
    "            # TODO: Your Code Here\n",
    "\n",
    "    prof.export_chrome_trace(f\"online_inf_{samples_num}_trace.json\")\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sample Profiling - Online Inference:\n",
    "\n",
    "# Vary this from 10, 100, 1000 to simulate multiple online inference\n",
    "loop_index = [10, 100, 1000]\n",
    "\n",
    "for idx in loop_index:\n",
    "    # Starts Profile logging\n",
    "    with profile(activities=activities) as prof:\n",
    "        with record_function(f\"e2e_online_{samples_num}\"):\n",
    "            # Actual online inference\n",
    "            # TODO: Your Code Here\n",
    "\n",
    "    prof.export_chrome_trace(f\"online_inf_{samples_num}_trace.json\")\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sample Profiling - Batch Inference:\n",
    "\n",
    "# We would only perform batch inference for a subset of validation set i.e. 1000 images \n",
    "# using different batch sizes of 20, 40, 100, 200 \n",
    "\n",
    "# Decides the size of the batch. Try: 20, 40, 100, 200\n",
    "batch_size = [20, 40, 100, 200]\n",
    "\n",
    "for size in batch_sizes:\n",
    "    val_dataloader = DataLoader(ds_val, batch_size=size, shuffle=True)\n",
    "\n",
    "    # Engage profiling\n",
    "    with profile(activities=activities) as prof:\n",
    "        with record_function(f\"e2e_batch_{size}\"):\n",
    "            # Actual Batch inference\n",
    "            # TODO: Your Code Here\n",
    "\n",
    "    prof.export_chrome_trace(f\"batch_inf_{size}_trace.json\")\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Training Options\n",
    "\n",
    "Important! Use the GPU environment for running code beyond this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = [ProfilerActivity.CUDA]\n",
    "\n",
    "models_dir = os.path.abspath('trained_models')\n",
    "pathlib.Path(models_dir).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# NOTE: This part is also definitely more verbose\n",
    "\n",
    "def train_network(dataloader, epochs, batch_size, train_type):\n",
    "    type_dir = os.path.join(models_dir, train_type)\n",
    "    pathlib.Path(type_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # create a new model and initialize it\n",
    "    model = TinyImageNetModel(lr=.0003, device=device).to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    #return the optimizer (adamax)\n",
    "    opt = model.configure_optimizers()\n",
    "    summary(model, input_size=(3, 64, 64))\n",
    "    print(f\"BEGINNING INFERENCE -- BATCH_SIZE={batch_size} FOR {epochs} EPOCHS\")\n",
    "    \n",
    "    # enable gradient computation\n",
    "    model.train()\n",
    "    i = 0\n",
    "    epoch_loss = 0.\n",
    "    epoch_accuracy = 0.\n",
    "\n",
    "    # begin the training loop\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time() * 1000  # Convert to milliseconds\n",
    "        print(f'EPOCH: {epoch}')\n",
    "        \n",
    "        for data, labels in dataloader:\n",
    "            loss, accuracy = model.training_step((data, labels))\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            i += 1\n",
    "            del loss\n",
    "            del accuracy\n",
    "\n",
    "        gc.collect()\n",
    "        end_time = time.time() * 1000\n",
    "        elapsed_time = (end_time - start_time) / 1000.0\n",
    "        print(f\"Elapsed time for Epoch {epoch}: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Average loss for Epoch {epoch}: {epoch_loss / i}\\t Train accuracy: {epoch_accuracy / i}\")\n",
    "\n",
    "\n",
    "    # Save the model at the end of each training run\n",
    "    torch.save(model, os.path.join(type_dir, f'{train_type}_train_{batch_size}_{epochs}.pt'))\n",
    "    # print(prof.key_averages().table(row_limit=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Different Batch Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Attempt to train your own model with different batch sizes\n",
    "\n",
    "# Move the whole dataset to the GPU before training\n",
    "gpu_dataset = GPUDS(ds_train, device)\n",
    "\n",
    "epoch_size = 20\n",
    "\n",
    "for batch_size in [32, 64, 128]:\n",
    "    # Setup our batched datasets\n",
    "    # Use timer to track training time\n",
    "    # TODO: Your Code Here\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Different Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train your model with 3 different numbers of epochs\n",
    "batch_size = 32\n",
    "\n",
    "# Setup your datasets\n",
    "# TODO: Your Code Here\n",
    "\n",
    "for epoch_size in [3, 10, 100]:\n",
    "    # Run training\n",
    "    # TODO: Your Code Here\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the Newly Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # TODO: Get the top-1 and top-5 of your newly trained model using different BATCHES\n",
    "    # TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # TODO: Get the top-1 and top-5 of your newly trained model using different EPOCHS\n",
    "    # TODO: Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Above and Beyond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add sections below for optional content in section 4 of the lab manual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
