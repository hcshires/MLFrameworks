{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Recipes – Fundamentals of ML\n",
    "\n",
    "## *Henry Shires, adapted from Google Developers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites (see .\\requirements.txt)\n",
    "- Python 3.6 or later\n",
    "- [Scikit-Learn](https://scikit-learn.org)\n",
    "- [TensorFlow](https://tensorflow.org)\n",
    "- NumPy\n",
    "- MatPlotLib\n",
    "\n",
    "### Resources\n",
    "- [Introduction to Machine Learning](https://towardsdatascience.comintroduction-to-machine-learning-db7c668822c4)\n",
    "- [Google Developers ML Recipes](https://www.youtube.com/playlist?list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal)\n",
    "- [Unity ML Agents](https://www.youtube.com/watch?v=32wtJZ3yRfw&index=2&list=PLX2vGYjWbI0R08eWQkO7nQkGiicHAX7IX&t=0s)\n",
    "- [Neural Networks Playground](https://playground.tensorflow.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe 1: Simple Classifier - Apples and Oranges\n",
    "\n",
    "Hello World (Determining the difference between an apple and an orange):\n",
    "\n",
    "- **Classifier:** A function that inputs data and assigns it a label as an output. This uses algorithms such as <mark>Supervised Learning</mark>. A classifier is like a “box of rules” determined by Training Data and gives an output based on a set of rules.\n",
    "- **Supervised Learning:** A technique to write a classifier automatically. It creates it by finding patterns in objects and data inputs. Libraries that include this functionality for developers are scikit-learn\n",
    "- This script performs the basic process for applying a machine learning algorithm to a dataset:\n",
    "- The four steps are:\n",
    "    - 1. Download a dataset (using pandas)\n",
    "    - 2. Process the numeric data (using NumPy)\n",
    "    - 3. Train and evaluate learners (using scikit-learn or TensorFlow)\n",
    "    - 4. Plot and compare results (using matplotlib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Training data\n",
    "# Input, 0 = bumpy, 1 = smooth\n",
    "features = [[140, 1], [130, 1], [150, 0], [170, 0]]\n",
    "# Output, 0 = apple, 1 = orange\n",
    "labels = [0, 0, 1, 1]\n",
    "\n",
    "# Defines type of classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Defines our Learning Algorithm (fit is for \"Finding patterns in Data\")\n",
    "clf = clf.fit(features, labels)\n",
    "\n",
    "# Inference\n",
    "p = clf.predict([[90, 0]])\n",
    "\n",
    "if p == 0: print(\"Apple\")\n",
    "elif p == 1: print(\"Orange\")\n",
    "else: print(\"Error predicting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[5.1 3.5 1.4 0.2] 0\n"
     ]
    }
   ],
   "source": [
    "# 1. import data set\n",
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "'''\n",
    "# print info from set\n",
    "print(iris.feature_names) # metadata\n",
    "print(iris.target_names)\n",
    "print(iris.data[0]) # features\n",
    "print(iris.target[0]) # labels\n",
    "'''\n",
    "\n",
    "testIdx = [0, 50, 100]  # each index where a new set of flowers begin\n",
    "\n",
    "# training data\n",
    "trainTarget = np.delete(iris.target, testIdx)\n",
    "trainData = np.delete(iris.data, testIdx, axis=0)\n",
    "\n",
    "# testing data\n",
    "testTarget = iris.target[testIdx]\n",
    "testData = iris.data[testIdx]\n",
    "\n",
    "# classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(trainData, trainTarget)\n",
    "\n",
    "print(testTarget)  # what our label is expected to be\n",
    "print(clf.predict(testData))\n",
    "\n",
    "print(testData[0], testTarget[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "greyhounds = 500  # sample population\n",
    "labs = 500\n",
    "\n",
    "greyHeight = 28 + 4 * np.random.randn(\n",
    "    greyhounds)  # change to make it realistic for actual data, but random ints added to increase variation\n",
    "labHeight = 24 + 4 * np.random.randn(labs)  # random adds height to entire population\n",
    "\n",
    "plt.hist([greyHeight, labHeight], stacked=True, color=['r', 'b'])  # Histogram to represent data\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n",
      "['setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor', 'virginica', 'setosa', 'versicolor', 'setosa', 'versicolor', 'virginica', 'virginica', 'versicolor', 'virginica', 'setosa', 'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor', 'virginica', 'setosa', 'setosa', 'virginica', 'virginica', 'versicolor', 'setosa', 'versicolor', 'setosa', 'versicolor', 'virginica', 'versicolor', 'versicolor', 'virginica', 'virginica', 'versicolor', 'virginica', 'versicolor', 'virginica', 'setosa', 'virginica', 'virginica', 'versicolor', 'versicolor', 'setosa', 'virginica', 'setosa', 'setosa', 'virginica', 'setosa', 'virginica', 'setosa', 'versicolor', 'virginica', 'versicolor', 'setosa', 'virginica', 'versicolor', 'versicolor', 'virginica', 'setosa', 'setosa', 'setosa', 'versicolor', 'virginica', 'versicolor', 'virginica', 'setosa', 'setosa', 'versicolor', 'virginica', 'virginica', 'setosa', 'setosa', 'versicolor']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ncustomData = [5.1, 3.5, 1.4, 0.2]\\ncustomPredict = clf.predict(customData)\\ncustomResults = results(customPredict)\\nprint(customResults)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def euc(a, b):\n",
    "    return distance.euclidean(a, b)  # calculate point distance\n",
    "\n",
    "\n",
    "class ScrappyKNN():  # Our classifier (Scrappy = bare-bones)\n",
    "    def fit(self, x_train, y_train):  # takes the features and labels as input\n",
    "        self.x_train = x_train  # features\n",
    "        self.y_train = y_train  # labels\n",
    "\n",
    "    def predict(self, x_test):  # inputs the features from testing data and outputs predictions\n",
    "        predictions = []  # returning list of predictions\n",
    "        for row in x_test:\n",
    "            label = self.closest(row)  # finds closest training point from test point\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "\n",
    "    def closest(self, row):  # loops over all of training points and updates to the closest one so far\n",
    "        bestDist = euc(row, self.x_train[0])  # records best distance from testing point to training point\n",
    "        bestIndex = 0  # records the best index\n",
    "        for i in range(len(self.x_train)):\n",
    "            dist = euc(row, self.x_train[i])\n",
    "            if dist < bestDist:\n",
    "                bestDist = dist\n",
    "                bestIndex = i\n",
    "        return self.y_train[bestIndex]\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x = iris.data  # features\n",
    "y = iris.target  # labels\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# a built in function that allows for automated train/test splitting of the data.\n",
    "# features and label values are used, and 'test_size' defines how much of the data is split into train or test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.5)\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier # train a new type of classifier\n",
    "clf = ScrappyKNN()  # sets classifier var to class\n",
    "\n",
    "\n",
    "def results(prediction):  # converts results to readable strings\n",
    "    data = prediction\n",
    "    for i in range(len(data)):\n",
    "        if predict[i] == 0:\n",
    "            data[i] = \"setosa\"\n",
    "        elif predict[i] == 1:\n",
    "            data[i] = \"versicolor\"\n",
    "        elif predict[i] == 2:\n",
    "            data[i] = \"virginica\"\n",
    "    return data\n",
    "\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "predict = clf.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, predict))\n",
    "\n",
    "results = results(predict)\n",
    "print(results)  # this prints the type of iris the classifier predicts for each row of testing data\n",
    "\n",
    "'''\n",
    "customData = [5.1, 3.5, 1.4, 0.2]\n",
    "customPredict = clf.predict(customData)\n",
    "customResults = results(customPredict)\n",
    "print(customResults)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m learn \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrib\u001b[49m\u001b[38;5;241m.\u001b[39mlearn\n\u001b[0;32m      8\u001b[0m tf\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mset_verbosity(tf\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mERROR)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Import the dataset\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "# CODE FORKED FROM @TENSORFLOW/TENSORFLOW. DO NOT DISTRIBUTE WITHOUT EDITING.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "learn = tf.contrib.learn\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Import the dataset\n",
    "mnist = learn.datasets.load_dataset('mnist')\n",
    "data = mnist.train.images\n",
    "labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "test_data = mnist.test.images\n",
    "test_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "max_examples = 10000\n",
    "data = data[:max_examples]\n",
    "labels = labels[:max_examples]\n",
    "\n",
    "\n",
    "# Display some digits\n",
    "def display(i):\n",
    "    img = test_data[i]\n",
    "    plt.title('Example %d. Label: %d' % (i, test_labels[i]))\n",
    "    plt.imshow(img.reshape((28, 28)), cmap=plt.cm.gray_r)\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    display(i)\n",
    "\n",
    "# lists features\n",
    "print(len(data[0]))\n",
    "\n",
    "# Fit a Linear Classifier\n",
    "feature_columns = learn.infer_real_valued_columns_from_input(data)\n",
    "classifier = learn.LinearClassifier(feature_columns=feature_columns, n_classes=10)\n",
    "classifier.fit(data, labels, batch_size=100, steps=1000)\n",
    "\n",
    "# Evaluate accuracy\n",
    "results = classifier.evaluate(test_data, test_labels)\n",
    "print(results, \"Accuracy\")\n",
    "\n",
    "predict = classifier.predict((test_data[0]), (test_labels[0]))\n",
    "\n",
    "# here's one it gets right\n",
    "print(\"Predicted %d, Label: %d\" % (predict))\n",
    "display(0)\n",
    "# one it gets wrong\n",
    "predict = classifier.predict((test_data[8]), (test_labels[8]))\n",
    "\n",
    "print(\"Predicted %d, Label: %d\" % (predict))\n",
    "display(8)\n",
    "\n",
    "# Let's see if we can reproduce the pictures of the weights in the TensorFlow Basic MNSIT\n",
    "weights = classifier.weights_\n",
    "f, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "axes = axes.reshape(-1)\n",
    "for i in range(len(axes)):\n",
    "    a = axes[i]\n",
    "    a.imshow(weights.T[i].reshape(28, 28), cmap=plt.cm.seismic)\n",
    "    a.set_title(i)\n",
    "    a.set_xticks(())  # ticks be gone\n",
    "    a.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is color == Green?\n",
      "Is diameter >= 3?\n",
      "True\n",
      "0.0\n",
      "0.5\n",
      "0.7999999999999998\n",
      "0.5714285714285715\n",
      "Is diameter >= 3?\n",
      "<__main__.Decision_Node object at 0x00000148FA2A6690>\n",
      "{'Apple': 3}\n",
      "{'Apple': '100%'}\n",
      "Actual: Apple. Predicted: {'Apple': '100%'}\n",
      "Actual: Apple. Predicted: {'Apple': '50%', 'Lemon': '50%'}\n",
      "Actual: Grape. Predicted: {'Grape': '100%'}\n",
      "Actual: Grape. Predicted: {'Grape': '100%'}\n",
      "Actual: Lemon. Predicted: {'Apple': '50%', 'Lemon': '50%'}\n"
     ]
    }
   ],
   "source": [
    "# For Python 2 / 3 compatability\n",
    "from __future__ import print_function\n",
    "\n",
    "# Toy dataset.\n",
    "# Format: each row is an example.\n",
    "# The last column is the label.\n",
    "# The first two columns are features.\n",
    "training_data = [\n",
    "    [\"Green\", 3, \"Apple\"],\n",
    "    [\"Yellow\", 3, \"Apple\"],\n",
    "    [\"Red\", 1, \"Grape\"],\n",
    "    [\"Red\", 1, \"Grape\"],\n",
    "    [\"Yellow\", 3, \"Lemon\"],\n",
    "    [\"Red\", 3, \"Apple\"],\n",
    "    [\"Red\", 3, \"Apple\"],\n",
    "]\n",
    "\n",
    "# print the tree\n",
    "header = [\"color\", \"diameter\", \"label\"]\n",
    "\n",
    "\n",
    "# print(header)\n",
    "\n",
    "\n",
    "def unique_vals(rows, col):\n",
    "    # Find the unique values for a column in a dataset.\n",
    "    return set([row[col] for row in rows])\n",
    "\n",
    "\n",
    "# unique_vals(training_data, 0)\n",
    "# returns column 0 of training data\n",
    "\n",
    "def class_counts(rows):\n",
    "    # Counts the number of each type of example in a dataset.\n",
    "    counts = {}  # a dictionary of label -> count.\n",
    "    for row in rows:\n",
    "        # in our dataset format, the label is always the last column\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "# class_counts(training_data)\n",
    "# returns how many times a label is present\n",
    "\n",
    "def is_numeric(value):\n",
    "    \"\"\"Test if a value is numeric. (E.g Distinguish color names from diameter)\"\"\"\n",
    "    return isinstance(value, int) or isinstance(value, float)\n",
    "\n",
    "\n",
    "# Demo\n",
    "# is_numeric(7)\n",
    "\n",
    "# Represents how a question is determined\n",
    "class Question:\n",
    "    \"\"\"A Question is used to partition a dataset.\n",
    "\n",
    "    This class just records a 'column number' (e.g., 0 for Color) and a\n",
    "    'column value' (e.g., Green). The 'match' method is used to compare\n",
    "    the feature value in an example to the feature value stored in the\n",
    "    question. See the demo below.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column, value):\n",
    "        # stores for the threshold used to partition the data\n",
    "        self.column = column  # string of type of feature\n",
    "        self.value = value  # value of the given node in the table\n",
    "\n",
    "    def match(self, example):\n",
    "        # Compare the feature value in an example to the\n",
    "        # feature value in this question.\n",
    "        val = example[self.column]\n",
    "        # print(val) prints the feature at self.column and the given index of training_data\n",
    "        if is_numeric(val):  # returns boolean values\n",
    "            return val >= self.value\n",
    "        else:\n",
    "            return val == self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        # This is just a helper method to print\n",
    "        # the question in a readable format.\n",
    "        condition = \"==\"\n",
    "        if is_numeric(self.value):\n",
    "            condition = \">=\"\n",
    "        return \"Is %s %s %s?\" % (\n",
    "            header[self.column], condition, str(self.value))\n",
    "\n",
    "\n",
    "# Prints example questions\n",
    "q = Question(0, 'Green')  # Is color == Green?\n",
    "print(q)\n",
    "print(Question(1, 3))  # Is diameter >= 3?\n",
    "\n",
    "# An example from the training set to see if it matches the question\n",
    "example = training_data[0]\n",
    "print(q.match(example))  # this will be true, since the first example is Green.\n",
    "\n",
    "\n",
    "def partition(rows, question):\n",
    "    \"\"\"Partitions a dataset.\n",
    "\n",
    "    For each row in the dataset, check if it matches the question. If\n",
    "    so, add it to 'true rows', otherwise, add it to 'false rows'.\n",
    "    \"\"\"\n",
    "    true_rows, false_rows = [], []\n",
    "    for row in rows:  # If the question is true for the given data, place it in true, otherwise it's false\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "    return true_rows, false_rows\n",
    "\n",
    "\n",
    "# Partition the training data based on whether rows are Red.\n",
    "true_rows, false_rows = partition(training_data, Question(0, 'Red'))\n",
    "\n",
    "\n",
    "# print(true_rows) Contains rows with Red\n",
    "\n",
    "def gini(rows):\n",
    "    \"\"\"Calculate the Gini Impurity for a list of rows.\n",
    "\n",
    "    There are a few different ways to do this, this one was\n",
    "    the most concise. See:\n",
    "    https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n",
    "    \"\"\"\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl ** 2\n",
    "    return impurity\n",
    "\n",
    "\n",
    "# First, we'll look at a dataset with no mixing.\n",
    "no_mixing = [['Apple'],\n",
    "             ['Apple']]\n",
    "# this will return 0\n",
    "print(gini(no_mixing))\n",
    "\n",
    "some_mixing = [['Apple'],\n",
    "               ['Orange']]\n",
    "# this will return 0.5 - meaning, there's a 50% chance of misclassifying\n",
    "# a random example we draw from the dataset.\n",
    "print(gini(some_mixing))\n",
    "\n",
    "lots_of_mixing = [['Apple'],\n",
    "                  ['Orange'],\n",
    "                  ['Grape'],\n",
    "                  ['Grapefruit'],\n",
    "                  ['Blueberry']]\n",
    "# This will return 0.8\n",
    "print(gini(lots_of_mixing))\n",
    "\n",
    "\n",
    "def info_gain(left, right, current_uncertainty):\n",
    "    \"\"\"Information Gain.\n",
    "\n",
    "    The uncertainty of the starting node, minus the weighted impurity of\n",
    "    two child nodes.\n",
    "    \"\"\"\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_uncertainty - p * gini(left) - (1 - p) * gini(right)\n",
    "\n",
    "\n",
    "# Calculate the uncertainy of our training data.\n",
    "current_uncertainty = gini(training_data)\n",
    "print(current_uncertainty)\n",
    "\n",
    "\n",
    "def find_best_split(rows):\n",
    "    \"\"\"Find the best question to ask by iterating over every feature / value\n",
    "    and calculating the information gain.\"\"\"\n",
    "    best_gain = 0  # keep track of the best information gain\n",
    "    best_question = None  # keep train of the feature / value that produced it\n",
    "    current_uncertainty = gini(rows)\n",
    "    n_features = len(rows[0]) - 1  # number of columns\n",
    "\n",
    "    for col in range(n_features):  # for each feature\n",
    "\n",
    "        values = set([row[col] for row in rows])  # unique values in the column\n",
    "\n",
    "        for val in values:  # for each value\n",
    "\n",
    "            question = Question(col, val)\n",
    "\n",
    "            # try splitting the dataset\n",
    "            true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "            # Skip this split if it doesn't divide the\n",
    "            # dataset.\n",
    "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate the information gain from this split\n",
    "            gain = info_gain(true_rows, false_rows, current_uncertainty)\n",
    "\n",
    "            # You actually can use '>' instead of '>=' here\n",
    "            # but I wanted the tree to look a certain way for our\n",
    "            # toy dataset.\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_question = gain, question\n",
    "\n",
    "    return best_gain, best_question\n",
    "\n",
    "\n",
    "# Find the best question to ask first for our toy dataset.\n",
    "best_gain, best_question = find_best_split(training_data)\n",
    "print(best_question)\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    \"\"\"A Leaf node classifies data.\n",
    "\n",
    "    This holds a dictionary of class (e.g., \"Apple\") -> number of times\n",
    "    it appears in the rows from the training data that reach this leaf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = class_counts(rows)\n",
    "\n",
    "\n",
    "class Decision_Node:\n",
    "    \"\"\"A Decision Node asks a question.\n",
    "\n",
    "    This holds a reference to the question, and to the two child nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 true_branch,\n",
    "                 false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "\n",
    "\n",
    "def build_tree(rows):\n",
    "    \"\"\"Builds the tree.\n",
    "\n",
    "    Rules of recursion: 1) Believe that it works. 2) Start by checking\n",
    "    for the base case (no further information gain). 3) Prepare for\n",
    "    giant stack traces.\n",
    "    \"\"\"\n",
    "\n",
    "    # Try partitioing the dataset on each of the unique attribute,\n",
    "    # calculate the information gain,\n",
    "    # and return the question that produces the highest gain.\n",
    "    gain, question = find_best_split(rows)\n",
    "\n",
    "    # Base case: no further info gain\n",
    "    # Since we can ask no further questions,\n",
    "    # we'll return a leaf.\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "\n",
    "    # If we reach here, we have found a useful feature / value\n",
    "    # to partition on.\n",
    "    true_rows, false_rows = partition(rows, question)\n",
    "\n",
    "    # Recursively build the true branch.\n",
    "    true_branch = build_tree(true_rows)\n",
    "\n",
    "    # Recursively build the false branch.\n",
    "    false_branch = build_tree(false_rows)\n",
    "\n",
    "    # Return a Question node.\n",
    "    # This records the best feature / value to ask at this point,\n",
    "    # as well as the branches to follow\n",
    "    # dependingo on the answer.\n",
    "    return Decision_Node(question, true_branch, false_branch)\n",
    "\n",
    "\n",
    "# Recieves the entire training set as input\n",
    "my_tree = build_tree(training_data)\n",
    "print(my_tree)\n",
    "\n",
    "\n",
    "def classify(row, node):\n",
    "    \"\"\"See the 'rules of recursion' above.\"\"\"\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.predictions\n",
    "\n",
    "    # Decide whether to follow the true-branch or the false-branch.\n",
    "    # Compare the feature / value stored in the node,\n",
    "    # to the example we're considering.\n",
    "    if node.question.match(row):\n",
    "        return classify(row, node.true_branch)\n",
    "    else:\n",
    "        return classify(row, node.false_branch)\n",
    "\n",
    "\n",
    "# training data is an apple with confidence 1.\n",
    "print(classify(training_data[0], my_tree))\n",
    "\n",
    "\n",
    "def print_leaf(counts):\n",
    "    \"\"\"A nicer way to print the predictions at a leaf.\"\"\"\n",
    "    total = sum(counts.values()) * 1.0\n",
    "    probs = {}\n",
    "    for lbl in counts.keys():\n",
    "        probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
    "    return probs\n",
    "\n",
    "\n",
    "# Printing that a bit nicer\n",
    "print(print_leaf(classify(training_data[0], my_tree)))\n",
    "\n",
    "# Evaluate\n",
    "testing_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 4, 'Apple'],\n",
    "    ['Red', 2, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "]\n",
    "\n",
    "for row in testing_data:\n",
    "    print(\"Actual: %s. Predicted: %s\" %\n",
    "          (row[-1], print_leaf(classify(row, my_tree))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
